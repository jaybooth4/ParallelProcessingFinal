{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(data, \n",
    "             folds, \n",
    "             reg = 1.0, \n",
    "             d = 10, \n",
    "             outputfile = 'output', \n",
    "             iter_num = 20, \n",
    "             N = 40, \n",
    "             master = 'local[40]'):\n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['main',\n",
    "                str(data),\n",
    "                str(folds),\n",
    "                '--reg', str(reg),\n",
    "                '--d', str(d),\n",
    "                '--outputfile', str(outputfile),\n",
    "                '--iter', str(iter_num),\n",
    "                '--N', str(N)]\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Alternating least squares.', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('data', help='Directory containing folds. The folds should be named fold0, fold1, ..., foldK.')\n",
    "    parser.add_argument('folds', type=int, help='Number of folds')\n",
    "    parser.add_argument('--reg', default=1.0, type=float, help=\"Regularization parameter\")\n",
    "    parser.add_argument('--d', default=10, type=int, help=\"Number of latent features\")\n",
    "    parser.add_argument('--outputfile', help='Output file to save the model')\n",
    "    parser.add_argument('--iter', default=20, type=int, help='Number of iterations to use during training')\n",
    "    parser.add_argument('--N', default=40, type=int, help='Parallelization Level')\n",
    "\n",
    "    verbosity_group = parser.add_mutually_exclusive_group(required=False)\n",
    "    verbosity_group.add_argument('--verbose', dest='verbose', action='store_true')\n",
    "    verbosity_group.add_argument('--silent', dest='verbose', action='store_false')\n",
    "    parser.set_defaults(verbose=False)\n",
    "\n",
    "    return parser.parse_args()\n",
    "    return parser.parse_args()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRatings(fileName, sparkContext, sess):\n",
    "    \"\"\" \n",
    "        Read in ratings from a given file. \n",
    "        Assumes the file is a csv in the format (user, item, rating)\n",
    "    \"\"\"\n",
    "    ratingsRDD = sparkContext.textFile(fileName).map(lambda x: tuple(x.split(',')))\\\n",
    "        .map(lambda (user, item, rating): Row(userId=int(user), itemId=int(item), rating=float(rating)))\n",
    "    return sess.createDataFrame(ratingsRDD)\n",
    "\n",
    "def readFolds(directory, numFolds, sc, sess):\n",
    "    \"\"\"\n",
    "        Reads folds of data from a directory.\n",
    "        Assumes files are formatted with \"fold[i]\" where i is the fold number\n",
    "    \"\"\"\n",
    "    folds = {}\n",
    "    for k in range(numFolds):\n",
    "        folds[k] = readRatings(directory+\"/fold\"+str(k), sc, sess).persist()\n",
    "    return folds\n",
    "\n",
    "def createTrainTestData(folds, k, N):\n",
    "    \"\"\"\n",
    "        Generates test and train data with the given fold object and value for k.\n",
    "        k represents the fold to use for testing, while all other folds will be used for training.\n",
    "    \"\"\"\n",
    "    train_folds = [folds[j] for j in folds if j is not k]\n",
    "    train = train_folds[0]\n",
    "    for fold in train_folds[1:]:\n",
    "        train = train.union(fold)\n",
    "    train = train.repartition(N).cache()\n",
    "    test = folds[k].repartition(N).cache()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, sc, sess, folds):\n",
    "#     folds = readFolds(args.data, args.folds, sc, sess)\n",
    "    cross_val_rmses = []\n",
    "    for k in range(len(folds)):\n",
    "        train, test = createTrainTestData(folds, k, args.N)\n",
    "        print\"Initiating fold %d with %d train samples and %d test samples\" % (k, train.count(), test.count())\n",
    "        train.persist()\n",
    "        test.persist()\n",
    "        start = time()\n",
    "        als = ALS(numUserBlocks=args.N,\n",
    "                  numItemBlocks=args.N,\n",
    "                  maxIter=args.iter,\n",
    "                  rank=args.d,\n",
    "                  regParam=args.reg, \n",
    "                  userCol=\"userId\", \n",
    "                  itemCol=\"itemId\", \n",
    "                  ratingCol=\"rating\", \n",
    "                  coldStartStrategy='drop')\n",
    "        model = als.fit(train)\n",
    "        predictions = model.transform(test)\n",
    "#         predictions.show()\n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "        testRMSE = evaluator.evaluate(predictions)\n",
    "\n",
    "        now = time()-start\n",
    "        print \"Fold: %d\\tTime: %f\\tTestRMSE: %f\" % (k, now, testRMSE)\n",
    "\n",
    "        cross_val_rmses.append(testRMSE)\n",
    "#         train.unpersist()\n",
    "#         test.unpersist()\n",
    "\n",
    "    print \"%d-fold cross validation error is: %f \" % (args.folds, np.mean(cross_val_rmses))\n",
    "    return np.mean(cross_val_rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '100g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '100g')\n",
    "try:\n",
    "    sc = SparkContext('local[40]', appName='Parallel MF')\n",
    "except:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    SparkContext.stop(sc)\n",
    "    sc = SparkContext('local[40]', appName='Parallel MF')\n",
    "spark = SparkSession(sc)\n",
    "sc.setLogLevel(\"ERROR\")   \n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "sess = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2eee9935a706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SparkContext.stop(sc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'100g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.driver.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'100g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark://10.99.248.36:7077\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Parallel MF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liu.che/.local/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36msetSystemProperty\u001b[0;34m(cls, key, value)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0minvoked\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0minstantiating\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liu.che/.local/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liu.che/.local/lib/python2.7/site-packages/pyspark/java_gateway.pyc\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SparkContext.stop(sc)\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '100g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '100g')\n",
    "sc = SparkContext(\"spark://10.99.248.36:7077\", appName='Parallel MF')\n",
    "sess = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "reg: 1, d: 2\n",
      "Initiating fold 0 with 303696 train samples and 303696 test samples\n",
      "Fold: 0\tTime: 39.266990\tTestRMSE: 1.177691\n",
      "Initiating fold 1 with 303696 train samples and 303696 test samples\n",
      "Fold: 1\tTime: 31.234609\tTestRMSE: 1.176208\n",
      "2-fold cross validation error is: 1.176949 \n"
     ]
    }
   ],
   "source": [
    "data = '../data/beer'\n",
    "num_folds = 2\n",
    "reg = 1.0\n",
    "d = 10\n",
    "outputfile = 'output'\n",
    "iter_num = 20\n",
    "N = 64\n",
    "folds = readFolds(data, num_folds, sc, sess)\n",
    "results = []\n",
    "for reg in range(1, 2):\n",
    "    for d in range(2, 3):\n",
    "        print('---------------------------------------')\n",
    "        print('reg: {}, d: {}'.format(reg, d))\n",
    "        args = get_args(data, num_folds, reg, d, outputfile, iter_num, N)\n",
    "        rms = train(args, sc, sess, folds)\n",
    "        results.append((reg, d, rms))\n",
    "        with open('../results/als/beer.pickle', 'wb+') as f:\n",
    "            pickle.dump(results, f)\n",
    "        with open('../results/als/beer.txt', 'ab+') as f:\n",
    "            f.write('{} {} {}\\n'.format(reg, d, rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
