{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(data, \n",
    "             folds, \n",
    "             reg = 1.0, \n",
    "             d = 10, \n",
    "             outputfile = 'output', \n",
    "             iter_num = 20, \n",
    "             N = 40):\n",
    "\n",
    "    sys.argv = ['main',\n",
    "                str(data),\n",
    "                str(folds),\n",
    "                '--reg', str(reg),\n",
    "                '--d', str(d),\n",
    "                '--outputfile', str(outputfile),\n",
    "                '--iter', str(iter_num),\n",
    "                '--N', str(N)]\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Alternating least squares.', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('data', help='Directory containing folds. The folds should be named fold0, fold1, ..., foldK.')\n",
    "    parser.add_argument('folds', type=int, help='Number of folds')\n",
    "    parser.add_argument('--reg', default=1.0, type=float, help=\"Regularization parameter\")\n",
    "    parser.add_argument('--d', default=10, type=int, help=\"Number of latent features\")\n",
    "    parser.add_argument('--outputfile', help='Output file to save the model')\n",
    "    parser.add_argument('--iter', default=20, type=int, help='Number of iterations to use during training')\n",
    "    parser.add_argument('--N', default=40, type=int, help='Parallelization Level')\n",
    "\n",
    "    verbosity_group = parser.add_mutually_exclusive_group(required=False)\n",
    "    verbosity_group.add_argument('--verbose', dest='verbose', action='store_true')\n",
    "    verbosity_group.add_argument('--silent', dest='verbose', action='store_false')\n",
    "    parser.set_defaults(verbose=False)\n",
    "\n",
    "    return parser.parse_args()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRatings(fileName, sparkContext, sess):\n",
    "    \"\"\" \n",
    "        Read in ratings from a given file. \n",
    "        Assumes the file is a csv in the format (user, item, rating)\n",
    "    \"\"\"\n",
    "    ratingsRDD = sparkContext.textFile(fileName).map(lambda x: tuple(x.split(',')))\\\n",
    "        .map(lambda (user, item, rating): Row(userId=int(user), itemId=int(item), rating=float(rating)))\n",
    "    return sess.createDataFrame(ratingsRDD)\n",
    "\n",
    "def readFolds(directory, numFolds, sc, sess):\n",
    "    \"\"\"\n",
    "        Reads folds of data from a directory.\n",
    "        Assumes files are formatted with \"fold[i]\" where i is the fold number\n",
    "    \"\"\"\n",
    "    folds = {}\n",
    "    if numFolds == 1:\n",
    "        numFolds = 5\n",
    "    for k in range(numFolds):\n",
    "        folds[k] = readRatings(directory+\"/fold_cleaned\"+str(k), sc, sess).persist()\n",
    "    return folds\n",
    "\n",
    "def createTrainTestData(folds, k, N, numFolds):\n",
    "    \"\"\"\n",
    "        Generates test and train data with the given fold object and value for k.\n",
    "        k represents the fold to use for testing, while all other folds will be used for training.\n",
    "    \"\"\"\n",
    "    if numFolds == 1:\n",
    "        k = -1\n",
    "    train_folds = [folds[j] for j in folds if j is not k]\n",
    "    train = train_folds[0]\n",
    "    for fold in train_folds[1:]:\n",
    "        train = train.union(fold)\n",
    "    train = train.repartition(N).cache()\n",
    "    if numFolds == 1:\n",
    "        test = train\n",
    "    else:\n",
    "        test = folds[k].repartition(N).cache()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, sc, sess, folds, numFolds):\n",
    "    cross_val_rmses = []\n",
    "    model = None\n",
    "    for k in range(numFolds):\n",
    "        train, test = createTrainTestData(folds, k, args.N, numFolds)\n",
    "        print\"Initiating fold %d with %d train samples and %d test samples\" % (k, train.count(), test.count())\n",
    "        train.persist()\n",
    "        test.persist()\n",
    "        start = time()\n",
    "        print('hello')\n",
    "        als = ALS(numUserBlocks=args.N,\n",
    "                  numItemBlocks=args.N,\n",
    "                  maxIter=args.iter,\n",
    "                  rank=args.d,\n",
    "                  regParam=args.reg, \n",
    "                  userCol=\"userId\", \n",
    "                  itemCol=\"itemId\", \n",
    "                  ratingCol=\"rating\", \n",
    "                  coldStartStrategy='drop')\n",
    "        print('hello2')\n",
    "        model = als.fit(train)\n",
    "        print('hello3')\n",
    "        predictions = model.transform(test)\n",
    "        print('hello4')\n",
    "#         predictions.show()\n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "        testRMSE = evaluator.evaluate(predictions)\n",
    "        now = time()-start\n",
    "        print \"Fold: %d\\tTime: %f\\tTestRMSE: %f\" % (k, now, testRMSE)\n",
    "        cross_val_rmses.append(testRMSE)\n",
    "    print \"%d-fold cross validation error is: %f \" % (args.folds, np.mean(cross_val_rmses))\n",
    "    return np.mean(cross_val_rmses), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext.stop(sc)\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '100g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '100g')\n",
    "sc = SparkContext('local[40]', appName='Parallel MF')\n",
    "# sc = SparkContext(\"spark://10.99.248.40:7077\", appName='Parallel MF')\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "sc.setLogLevel(\"ERROR\")   \n",
    "sess = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "reg: 0.08, d: 20\n",
      "Initiating fold 0 with 1217280 train samples and 309653 test samples\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "hello4\n"
     ]
    }
   ],
   "source": [
    "data = '../data/beer'\n",
    "outputfile = None\n",
    "\n",
    "num_folds = 5\n",
    "iter_num = 20\n",
    "N = 64\n",
    "folds = readFolds(data, num_folds, sc, sess)\n",
    "results = []\n",
    "\n",
    "reg = 0.08\n",
    "d = 20\n",
    "# for reg in range(1, 2):\n",
    "#     for d in range(2, 3):\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('reg: {}, d: {}'.format(reg, d))\n",
    "args = get_args(data, num_folds, reg, d, outputfile, iter_num, N)\n",
    "rms, model = train(args, sc, sess, folds, num_folds)\n",
    "# results.append((reg, d, rms))\n",
    "# with open('../results/als/beer_0.08_20.pickle', 'wb+') as f:\n",
    "#     pickle.dump(results, f)\n",
    "# with open('../results/als/beer_0.08_20.txt', 'ab+') as f:\n",
    "#     f.write('{} {} {}\\n'.format(reg, d, rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_str = model.userFactors.rdd.map(lambda (x, y): ','.join(str(t) for t in [x] + list(y)))\n",
    "v_str = model.itemFactors.rdd.map(lambda (x, y): ','.join(str(t) for t in [x] + list(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_str.saveAsTextFile('../results/als/u.csv')\n",
    "v_str.saveAsTextFile('../results/als/v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
